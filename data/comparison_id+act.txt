*******************************************
 CONFIGURATION: 
NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=BaseLayer(activationFn=identity, weightInit=XAVIER, biasInit=0.0, dist=null, l1=0.0, l2=0.0, l1Bias=0.0, l2Bias=0.0, iUpdater=AdaGrad(learningRate=18.4, learningRateSchedule=null, epsilon=1.0E-6), biasUpdater=null, weightNoise=null, gradientNormalization=None, gradientNormalizationThreshold=1.0), nIn=2, nOut=2), hasBias=true), miniBatch=true, maxNumLineSearchIterations=5, seed=123, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[0_W, 0_b, 1_W, 1_b, 2_W, 2_b, 3_W, 3_b, 4_W, 4_b], stepFunction=null, minimize=true, pretrain=false, cacheMode=NONE, iterationCount=0, epochCount=0)
*******************************************
*******************************************
 CONFIGURATION AFTER TRAINING: 
NeuralNetConfiguration(layer=DenseLayer(super=FeedForwardLayer(super=BaseLayer(activationFn=identity, weightInit=XAVIER, biasInit=0.0, dist=null, l1=0.0, l2=0.0, l1Bias=0.0, l2Bias=0.0, iUpdater=AdaGrad(learningRate=18.4, learningRateSchedule=null, epsilon=1.0E-6), biasUpdater=null, weightNoise=null, gradientNormalization=None, gradientNormalizationThreshold=1.0), nIn=2, nOut=2), hasBias=true), miniBatch=true, maxNumLineSearchIterations=5, seed=123, optimizationAlgo=STOCHASTIC_GRADIENT_DESCENT, variables=[0_W, 0_b, 1_W, 1_b, 2_W, 2_b, 3_W, 3_b, 4_W, 4_b], stepFunction=null, minimize=true, pretrain=false, cacheMode=NONE, iterationCount=0, epochCount=0)
*******************************************
Column    MSE            MAE            RMSE           RSE            PC             R^2            
col_0     4.07278e+02    1.59177e+01    2.01811e+01    1.02665e+00    3.54526e-01    -2.66532e-02   


